{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import Model \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'Fresh_Augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "image_datagen = ImageDataGenerator(rescale = 1./255., validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11749 images belonging to 12 classes.\n",
      "Found 2933 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = image_datagen.flow_from_directory(base_dir, batch_size = 20, shuffle = True, subset =\"training\", class_mode = \"categorical\",  target_size = (224, 224))\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = image_datagen.flow_from_directory(base_dir,  batch_size = 20, shuffle = True, subset =\"validation\", class_mode = \"categorical\", target_size = (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(base_model.output)\n",
    "\n",
    "# Add a fully connected layer with 512 hidden units and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Add a final sigmoid layer for classification\n",
    "x = layers.Dense(12, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'categorical_crossentropy',metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints which saves partial model at each epoch and stops the model if validatation loss isn't changing\n",
    "checkpoint = ModelCheckpoint(\"saved_model_temp.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only = True, verbose=1)\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3, verbose = 1, restore_best_weights = True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',factor = 0.2, patience = 3, verbose = 1, min_delta = 0.00001)\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "587/587 [==============================] - 137s 205ms/step - loss: 0.8882 - acc: 0.8006 - val_loss: 4.5402 - val_acc: 0.8970\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.54019, saving model to saved_model_temp.h5\n",
      "Epoch 2/20\n",
      "587/587 [==============================] - 114s 194ms/step - loss: 0.1422 - acc: 0.9785 - val_loss: 6.0808 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 4.54019\n",
      "Epoch 3/20\n",
      "587/587 [==============================] - 117s 199ms/step - loss: 0.1115 - acc: 0.9835 - val_loss: 41.2849 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.54019\n",
      "Epoch 4/20\n",
      "587/587 [==============================] - 114s 195ms/step - loss: 0.1115 - acc: 0.9857 - val_loss: 8.1831 - val_acc: 0.8762\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.54019\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "vgghist = model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 11749//20 , epochs = 20, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Modified_Dataset/saved_models/1621593225_Inceptionv3\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Modified_Dataset/saved_models/1621593225_Inceptionv3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "# Exporting Model in .pb format\n",
    "export_path = \"Modified_Dataset/saved_models/{}_Inceptionv3\".format(int(t))\n",
    "model.save(export_path)\n",
    "\n",
    "export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96037296"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting Model to tflite optimised model \n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_path)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quantized_model = converter.convert()\n",
    "open(export_path + \"/float16_optimised_model_Inceptionv3.tflite\", \"wb\").write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191993756"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting Model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_path)\n",
    "\n",
    "# tflite_model = converter.convert()\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quantized_model = converter.convert()\n",
    "open(export_path + \"/float16_model_Inceptionv3.tflite\", \"wb\").write(tflite_quantized_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}